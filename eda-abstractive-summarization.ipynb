{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0603 23:08:32.041300 139700335359808 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "tf.enable_resource_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values used in the base model of transformer were; num_layers=6, d_model = 512, dff = 2048. See the paper for all the other versions of the transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config\n",
    "\n",
    "\n",
    "BERT_MODEL_URL = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "SEQ_LEN = 3\n",
    "MAX_EXAMPLE_LEN = 512\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "BUFFER_SIZE = 1\n",
    "\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "# input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "# target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0603 23:08:33.138823 139700335359808 resolver.py:79] Using /tmp/tfhub_modules to cache modules.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0603 23:08:34.108809 139700335359808 deprecation.py:323] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0603 23:08:36.418521 139700335359808 saver.py:1489] Saver not created because there are no variables in the graph to restore\n",
      "I0603 23:08:36.957436 139700335359808 dataset_builder.py:526] No config specified, defaulting to first: cnn_dailymail/plain_text\n",
      "I0603 23:08:36.958978 139700335359808 dataset_builder.py:174] Overwrite dataset info from restored data version.\n",
      "I0603 23:08:36.962034 139700335359808 dataset_builder.py:216] Reusing dataset cnn_dailymail (/home/ec2-user/tensorflow_datasets/cnn_dailymail/plain_text/0.0.2)\n"
     ]
    }
   ],
   "source": [
    "from data.load import load_cnn_dailymail\n",
    "from ops.session import initialize_vars\n",
    "\n",
    "\n",
    "train_dataset, val_dataset, test_dataset, metadata = load_cnn_dailymail()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(b'/tmp/tfhub_modules/5a395eafef2a37bd9fc55d7f6ae676d2a134a838/assets/vocab.txt') as f:\n",
    "    data = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_ID = 100\n",
    "CLS_ID = 101\n",
    "SEP_ID = 102\n",
    "MASK_ID = 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_timestamp(x, i, mask_with):\n",
    "    \"\"\"\n",
    "    Masks each word in the summary draft one by one with the [MASK] token\n",
    "    At t-th time step the t-th word of input summary is\n",
    "    masked, and the decoder predicts the refined word given other\n",
    "    words of the summary.\n",
    "    \n",
    "    x :: (N, T)\n",
    "    returrn :: (N, T)\n",
    "    \"\"\"\n",
    "\n",
    "    N, T = tf.shape(x)[0], tf.shape(x)[1]\n",
    "\n",
    "    left = x[:, :i]\n",
    "    right = x[:, i+1:]\n",
    "    \n",
    "    mask = tf.ones([N, 1], dtype=x.dtype) * mask_with\n",
    "    \n",
    "    masked = tf.concat([left, mask, right], axis=1)\n",
    "\n",
    "    \n",
    "    return masked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(pred, real):\n",
    "    \"\"\"\n",
    "    Since the target sequences are padded,\n",
    "    it is important to apply a padding mask when calculating the loss.\n",
    "    \"\"\"\n",
    "    logits_draft_summary, logits_refined_summary = pred\n",
    "    \n",
    "    target_ids_ = label_smoothing(tf.one_hot(real, depth=self.vocab_size))\n",
    "\n",
    "    # use right shifted target\n",
    "    loss_draft = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits_draft_summary, labels=target_ids_[:, 1:])\n",
    "\n",
    "    # use non-shifted target (we want to predict the masked word)\n",
    "    loss_refined = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits_refined_summary, labels=target_ids_[:, :-1])\n",
    "\n",
    "    loss = loss_draft + loss_refined    \n",
    "    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    loss *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss)  \n",
    "\n",
    "\n",
    "warmup_steps = 4000\n",
    "initial_lr = 0.0003\n",
    "\n",
    "\n",
    "def noam_scheme(init_lr, global_step, warmup_steps=4000.):\n",
    "    '''Noam scheme learning rate decay\n",
    "    init_lr: initial learning rate. scalar.\n",
    "    global_step: scalar.\n",
    "    warmup_steps: scalar. During warmup_steps, learning rate increases\n",
    "        until it reaches init_lr.\n",
    "    '''\n",
    "    step = tf.cast(global_step + 1, dtype=tf.float32)\n",
    "    return init_lr * warmup_steps ** 0.5 * tf.minimum(step * warmup_steps ** -1.5, step ** -0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0603 14:59:45.152906 139751406184256 <ipython-input-10-94056157d725>:24] Extracting pretrained word embeddings weights from BERT\n",
      "I0603 14:59:50.853898 139751406184256 <ipython-input-10-94056157d725>:32] Embedding matrix shape '(30522, 768)'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.keras.initializers import Constant\n",
    "\n",
    "from layers.transformer import Encoder\n",
    "from layers.transformer import Decoder\n",
    "from layers.bert import BertLayer, BERT_MODEL_URL\n",
    "from layers.transformer import Decoder\n",
    "\n",
    "from ops.masking import create_masks\n",
    "from ops.regularization import label_smoothing\n",
    "\n",
    "\n",
    "def _embedding_from_bert():\n",
    "    \"\"\"\n",
    "    Extract the preratined word embeddings from a BERT model\n",
    "    Returns a numpy matrix with the embeddings\n",
    "    \"\"\"\n",
    "    logger.info(\"Extracting pretrained word embeddings weights from BERT\")\n",
    "    \n",
    "    bert = hub.Module(BERT_MODEL_URL, trainable=False, name=\"embeddings_from_bert_module\")    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        initialize_vars(sess)\n",
    "        embedding_matrix = sess.run(bert.variable_map['bert/embeddings/word_embeddings'])\n",
    "                \n",
    "    logger.info(f\"Embedding matrix shape '{embedding_matrix.shape}'\")\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "class AbstractiveSummarization(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Pretraining-Based Natural Language Generation for Text Summarization \n",
    "    https://arxiv.org/pdf/1902.09243.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, seq_len, rate=0.1):\n",
    "        super(AbstractiveSummarization, self).__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "        self.bert = BertLayer(seq_len=seq_len, d_embedding=d_model, trainable=False)\n",
    "        \n",
    "        embedding_matrix = _embedding_from_bert()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size, d_model, trainable=False,\n",
    "            embeddings_initializer=Constant(embedding_matrix)\n",
    "        )\n",
    "        \n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, vocab_size, rate)\n",
    "                \n",
    "        self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    def encode(self, ids, mask, segment_ids):\n",
    "        # (batch_size, seq_len, d_bert)\n",
    "        return self.bert((ids, mask, segment_ids))    \n",
    "    \n",
    "    def draft_summary(self, target_ids, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        \n",
    "        # (batch_size, seq_len, d_bert)\n",
    "        embeddings = self.embedding(target_ids)    \n",
    "\n",
    "        # (batch_size, seq_len, d_bert), (_)            \n",
    "        dec_output, attention_dist = self.decoder(embeddings, enc_output, training, look_ahead_mask, padding_mask)\n",
    "        \n",
    "        # (batch_size, seq_len, vocab_len)\n",
    "        logits = self.final_layer(dec_output)\n",
    "        \n",
    "        # (batch_size, seq_len)\n",
    "        preds = tf.to_int32(tf.argmax(logits, axis=-1))\n",
    "        \n",
    "        return logits, preds, attention_dist      \n",
    "    \n",
    "    def refined_summary(self, target_ids, target_mask, target_segment_ids, enc_output, draft_summary, training, padding_mask):\n",
    "        \n",
    "        dec_outputs, attention_dists = [], []\n",
    "\n",
    "        for i in range(0, self.seq_len):\n",
    "            \n",
    "            # (batch_size, seq_len)\n",
    "            target_ids = mask_timestamp(target_ids, i, MASK_ID)\n",
    "            \n",
    "            # (batch_size, seq_len, d_bert)\n",
    "            context_vectors = self.bert((target_ids, target_mask, target_segment_ids))\n",
    "            \n",
    "            # (batch_size, seq_len, d_bert), (_)\n",
    "            dec_output, attention_dist = self.decoder(\n",
    "                context_vectors,\n",
    "                enc_output,\n",
    "                training,\n",
    "                look_ahead_mask=None,\n",
    "                padding_mask=padding_mask\n",
    "            )\n",
    "            \n",
    "            # (batch_size, 1, vocab_len)\n",
    "            dec_outputs += [dec_output[:,i:i+1,:]]\n",
    "            attention_dists += [{k: v[:, i:i+1 :] for k, v in attention_dist.items()}]\n",
    "\n",
    "        # (batch_size, seq_len, d_bert)            \n",
    "        dec_outputs = tf.concat(dec_outputs, axis=1)\n",
    "                \n",
    "        # (batch_size, seq_len, vocab_len)\n",
    "        logits = self.final_layer(dec_outputs)    \n",
    "        \n",
    "        # (batch_size, seq_len)\n",
    "        preds = tf.to_int32(tf.argmax(logits, axis=-1))\n",
    "        \n",
    "        return logits, preds, attention_dists\n",
    "    \n",
    "    def call(self, inp, tar, training=True):\n",
    "        \n",
    "        # (batch_size, seq_len) x3\n",
    "        input_ids, input_mask, input_segment_ids = inp\n",
    "        \n",
    "        # (batch_size, seq_len + 1) x3\n",
    "        target_ids, target_mask, target_segment_ids = tar   \n",
    "                \n",
    "        # (batch_size, 1, 1, seq_len), (_), (batch_size, 1, 1, seq_len)\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(input_ids, target_ids[:, :-1])\n",
    "\n",
    "        # (batch_size, seq_len, d_bert)\n",
    "        enc_output = self.encode(input_ids, input_mask, input_segment_ids)\n",
    "        \n",
    "        # (batch_size, seq_len , vocab_len), (batch_size, seq_len), (_)\n",
    "        logits_draft_summary, preds_draft_summary, draft_attention_dist = self.draft_summary(\n",
    "            target_ids[:, :-1],\n",
    "            enc_output,\n",
    "            training,\n",
    "            combined_mask,\n",
    "            dec_padding_mask\n",
    "        )\n",
    "        \n",
    "        # (batch_size, seq_len, vocab_len), (batch_size, seq_len), (_)\n",
    "        logits_refined_summary, preds_refined_summary, refined_attention_dist = self.refined_summary(\n",
    "            target_ids[:, :-1],\n",
    "            target_mask[:, :-1],\n",
    "            target_segment_ids[:, :-1],\n",
    "            enc_output,\n",
    "            preds_draft_summary,\n",
    "            training,\n",
    "            dec_padding_mask\n",
    "        )\n",
    "        \n",
    "        return logits_draft_summary, logits_refined_summary\n",
    "    \n",
    "    \n",
    "    def train(self, xs, ys):\n",
    "        # (batch_size, seq_len + 1) x3\n",
    "        target_ids, _, _ = ys\n",
    "        \n",
    "        # (batch_size, seq_len, vocab_len), (batch_size, seq_len, vocab_len)\n",
    "        logits_draft_summary, logits_refined_summary = self(xs, ys, True)\n",
    "        \n",
    "        target_ids_ = label_smoothing(tf.one_hot(target_ids, depth=self.vocab_size))\n",
    "\n",
    "        # use right shifted target, (batch_size, seq_len)\n",
    "        loss_draft = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits_draft_summary, labels=target_ids_[:, 1:])\n",
    "        mask = tf.math.logical_not(tf.math.equal(target_ids_[:, 1:], 0))    \n",
    "        mask = tf.cast(mask, dtype=loss.dtype)\n",
    "        loss_draft *= mask\n",
    "        \n",
    "        # use non-shifted target (we want to predict the masked word), (batch_size, seq_len)\n",
    "        loss_refined = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits_refined_summary, labels=target_ids_[:, :-1])\n",
    "        mask = tf.math.logical_not(tf.math.equal(target_ids_[:, :-1], 0))    \n",
    "        mask = tf.cast(mask, dtype=loss.dtype)\n",
    "        loss_refined *= mask        \n",
    "\n",
    "        # (batch_size, seq_len)\n",
    "        loss = loss_draft + loss_refined            \n",
    "        # scalar\n",
    "        loss = tf.reduce_mean(loss)\n",
    "\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        learning_rate = noam_scheme(initial_lr, global_step, warmup_steps)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.98, epsilon=1e-9)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "        tf.summary.scalar('learning_rate', learning_rate)\n",
    "        tf.summary.scalar('loss_draft', tf.reduce_mean(loss_draft * mask))\n",
    "        tf.summary.scalar('loss_refined', tf.reduce_mean(loss_refined * mask))\n",
    "        tf.summary.scalar(\"loss\", loss)\n",
    "        tf.summary.scalar(\"global_step\", global_step)\n",
    "\n",
    "        summaries = tf.summary.merge_all()\n",
    "\n",
    "        return loss, train_op, global_step, summaries          \n",
    "\n",
    "    \n",
    "model = AbstractiveSummarization(\n",
    "    num_layers=8,\n",
    "    d_model=768,\n",
    "    num_heads=8,\n",
    "    dff=2048,\n",
    "    vocab_size=30522,\n",
    "    seq_len=SEQ_LEN,\n",
    "    rate=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0603 14:59:54.543257 139751406184256 saver.py:1489] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0603 15:00:00.144028 139751406184256 deprecation.py:506] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-94056157d725>:76: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0603 15:00:02.854254 139751406184256 deprecation.py:323] From <ipython-input-10-94056157d725>:76: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0603 15:00:03.997836 139751406184256 saver.py:1489] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0603 15:00:07.417161 139751406184256 saver.py:1489] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0603 15:00:11.253221 139751406184256 saver.py:1489] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "iterator = train_dataset.make_initializable_iterator()\n",
    "stream = iterator.get_next()\n",
    "xs, ys = stream[:3], stream[3:]\n",
    "\n",
    "loss, train_op, global_step, train_summaries = model.train(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert_layer (BertLayer)       multiple                  110104890 \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        multiple                  23440896  \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            multiple                  63023104  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  23471418  \n",
      "=================================================================\n",
      "Total params: 220,040,308\n",
      "Trainable params: 86,494,522\n",
      "Non-trainable params: 133,545,786\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    if os.path.isdir(config.LOGDIR):\n",
    "        shutil.rmtree(config.LOGDIR)\n",
    "    \n",
    "    summary_writer = tf.summary.FileWriter(config.LOGDIR, sess.graph)\n",
    "    \n",
    "    sess.run(iterator.initializer)\n",
    "    \n",
    "    initialize_vars(sess)\n",
    "    \n",
    "    _gs = sess.run(global_step)\n",
    "    \n",
    "    for i in range(15):\n",
    "        \n",
    "        _, _gs, _summary = sess.run([train_op, global_step, train_summaries])\n",
    "        \n",
    "        summary_writer.add_summary(_summary, _gs)\n",
    "        summary_writer.flush()        \n",
    "        \n",
    "    summary_writer.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-935522348baf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_accuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m       \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m               \u001b[0mweighted_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweighted_masked_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m               \u001b[0moutput_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweighted_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mweighted\u001b[0;34m(y_true, y_pred, weights, mask)\u001b[0m\n\u001b[1;32m    645\u001b[0m     \"\"\"\n\u001b[1;32m    646\u001b[0m     \u001b[0;31m# score_array has ndim >= 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m     \u001b[0mscore_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m       \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-ecfc6dd94308>\u001b[0m in \u001b[0;36mloss_function\u001b[0;34m(pred, real)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mimportant\u001b[0m \u001b[0mto\u001b[0m \u001b[0mapply\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0mmask\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mcalculating\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlogits_draft_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_refined_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtarget_ids_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m       raise TypeError(\n\u001b[0;32m--> 442\u001b[0;31m           \u001b[0;34m\"Tensor objects are only iterable when eager execution is \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m           \"enabled. To iterate over this tensor use tf.map_fn.\")\n\u001b[1;32m    444\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn."
     ]
    }
   ],
   "source": [
    "\n",
    "_gs\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "learning_rate = noam_scheme(initial_lr, global_step, warmup_steps)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.98, epsilon=1e-9)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_function,\n",
    "    metrics=[tf.keras.metrics.categorical_accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(model, tf.keras.Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Adam/iterations:0' shape=() dtype=int64>, <tf.Variable 'Adam/lr:0' shape=() dtype=float32>, <tf.Variable 'Adam/beta_1:0' shape=() dtype=float32>, <tf.Variable 'Adam/beta_2:0' shape=() dtype=float32>, <tf.Variable 'Adam/decay:0' shape=() dtype=float32>, <tf.Variable 'Adam_1/iterations:0' shape=() dtype=int64>, <tf.Variable 'Adam_1/lr:0' shape=() dtype=float32>, <tf.Variable 'Adam_1/beta_1:0' shape=() dtype=float32>, <tf.Variable 'Adam_1/beta_2:0' shape=() dtype=float32>, <tf.Variable 'Adam_1/decay:0' shape=() dtype=float32>, <tf.Variable 'Adam_2/iterations:0' shape=() dtype=int64>, <tf.Variable 'Adam_2/lr:0' shape=() dtype=float32>, <tf.Variable 'Adam_2/beta_1:0' shape=() dtype=float32>, <tf.Variable 'Adam_2/beta_2:0' shape=() dtype=float32>, <tf.Variable 'Adam_2/decay:0' shape=() dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(tf.trainable_variables())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 6 arrays: [<tf.Tensor 'IteratorGetNext_1:0' shape=(?, ?) dtype=int64>, <tf.Tensor 'IteratorGetNext_1:1' shape=(?, ?) dtype=int64>, <tf.Tensor 'IteratorGetNext_1:2' shape=(?, ?) dtype=int64>, <tf.Tensor 'Iterato...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-73404c0660db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_initializable_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[1;32m   2380\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m         exception_prefix='input')\n\u001b[0m\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    321\u001b[0m                        \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                        \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m                        str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    324\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m       raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 6 arrays: [<tf.Tensor 'IteratorGetNext_1:0' shape=(?, ?) dtype=int64>, <tf.Tensor 'IteratorGetNext_1:1' shape=(?, ?) dtype=int64>, <tf.Tensor 'IteratorGetNext_1:2' shape=(?, ?) dtype=int64>, <tf.Tensor 'Iterato..."
     ]
    }
   ],
   "source": [
    "iter = train_dataset.make_initializable_iterator()\n",
    "el = iter.get_next()\n",
    "model.fit(el, epochs=10, steps_per_epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not all arguments converted during string formatting",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3fd9c1e6963b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minitialize_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[1;32m   2237\u001b[0m               \u001b[0;34m'Please provide model inputs as a list or tuple of 2  or 3'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m               \u001b[0;34m'elements: (input, target) or (input, target, sample_weights)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2239\u001b[0;31m               'Received %s' % next_element)\n\u001b[0m\u001b[1;32m   2240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_element\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2241\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: not all arguments converted during string formatting"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    initialize_vars(sess)    \n",
    "    model.fit(train_dataset, epochs=1, steps_per_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?) (?, ?) (?, ?)\n",
      "(?, ?) (?, ?) (?, ?)\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0602 02:27:36.021267 140483075295040 saver.py:1489] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0602 02:27:37.085570 140483075295040 deprecation.py:506] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-92704cbc1b5f>:90: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0602 02:27:41.069661 140483075295040 deprecation.py:323] From <ipython-input-9-92704cbc1b5f>:90: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0602 02:27:42.188103 140483075295040 saver.py:1489] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0602 02:27:45.645426 140483075295040 saver.py:1489] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0602 02:27:49.526523 140483075295040 saver.py:1489] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0602 02:27:53.526570 140483075295040 saver.py:1489] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.578041]\n"
     ]
    }
   ],
   "source": [
    "iter = train_dataset.make_initializable_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iter.initializer)  \n",
    "#     tmp = sess.run(el)\n",
    "    x0, x1, x2, y0, y1, y2 = el\n",
    "    print(x0.shape, x1.shape, x2.shape)\n",
    "    print(y0.shape, y1.shape, y2.shape)\n",
    "    \n",
    "    a = model((x0,x1,x2), (y0,y1,y2), True)\n",
    "    \n",
    "    initialize_vars(sess)\n",
    "#     for i in m.trainable_weights: print(i)\n",
    "#     raise ValueError\n",
    "    \n",
    "    a = sess.run([a])\n",
    "    print(a)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.summary.scalar('lr', lr)\n",
    "# tf.summary.scalar(\"loss\", loss)\n",
    "# tf.summary.scalar(\"global_step\", global_step)\n",
    "\n",
    "# summaries = tf.summary.merge_all()\n",
    "\n",
    "warmup_steps = 4000\n",
    "initial_lr = 0.0003\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = model(inputs, targets)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "\n",
    "def noam_scheme(init_lr, global_step, warmup_steps=4000.):\n",
    "    '''Noam scheme learning rate decay\n",
    "    init_lr: initial learning rate. scalar.\n",
    "    global_step: scalar.\n",
    "    warmup_steps: scalar. During warmup_steps, learning rate increases\n",
    "        until it reaches init_lr.\n",
    "    '''\n",
    "    step = tf.cast(global_step + 1, dtype=tf.float32)\n",
    "    return init_lr * warmup_steps ** 0.5 * tf.minimum(step * warmup_steps ** -1.5, step ** -0.5)\n",
    "\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "learning_rate = noam_scheme(initial_lr, global_step, warmup_steps)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "\n",
    "iter = train_dataset.make_initializable_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iter.initializer)  \n",
    "#     tmp = sess.run(el)\n",
    "    x0, x1, x2, y0, y1, y2 = el\n",
    "    print(x0.shape, x1.shape, x2.shape)\n",
    "    print(y0.shape, y1.shape, y2.shape)\n",
    "    \n",
    "    loss_value, grads = grad(model, (x0,x1,x2), (y0,y1,y2))\n",
    "    \n",
    "    print(loss_value)\n",
    "    print(grads)\n",
    "\n",
    "    initialize_vars(sess)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_id = tf.keras.layers.Input(shape=(MAX_EXAMPLE_LEN,), name=\"input_ids\")\n",
    "in_mask = tf.keras.layers.Input(shape=(MAX_EXAMPLE_LEN,), name=\"input_masks\")\n",
    "in_segment = tf.keras.layers.Input(shape=(MAX_EXAMPLE_LEN,), name=\"segment_ids\")\n",
    "\n",
    "warmup_steps = 4000\n",
    "initial_lr = 0.0003\n",
    "\n",
    "def noam_scheme(init_lr, global_step, warmup_steps=4000.):\n",
    "    '''Noam scheme learning rate decay\n",
    "    init_lr: initial learning rate. scalar.\n",
    "    global_step: scalar.\n",
    "    warmup_steps: scalar. During warmup_steps, learning rate increases\n",
    "        until it reaches init_lr.\n",
    "    '''\n",
    "    step = tf.cast(global_step + 1, dtype=tf.float32)\n",
    "    return init_lr * warmup_steps ** 0.5 * tf.minimum(step * warmup_steps ** -1.5, step ** -0.5)\n",
    "\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "learning_rate = noam_scheme(initial_lr, global_step, warmup_steps)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0601 17:55:21.698049 140022164780864 deprecation.py:323] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Error while reading resource variable global_step from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/global_step)\n\t [[node global_step/Read/ReadVariableOp (defined at <ipython-input-4-76a7a9e69621>:3) ]]\n\nCaused by op 'global_step/Read/ReadVariableOp', defined at:\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-76a7a9e69621>\", line 3, in <module>\n    global_step = tf.train.get_or_create_global_step()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/training_util.py\", line 162, in get_or_create_global_step\n    global_step_tensor = create_global_step(graph)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/training_util.py\", line 145, in create_global_step\n    ops.GraphKeys.GLOBAL_STEP])\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1479, in get_variable\n    aggregation=aggregation)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1220, in get_variable\n    aggregation=aggregation)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 547, in get_variable\n    aggregation=aggregation)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 499, in _true_getter\n    aggregation=aggregation)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 911, in _get_single_variable\n    aggregation=aggregation)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 213, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 176, in _variable_v1_call\n    aggregation=aggregation)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 155, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2488, in default_variable_creator\n    import_scope=import_scope)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 217, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 294, in __init__\n    constraint=constraint)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 446, in _init_from_args\n    value = self._read_variable_op()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 728, in _read_variable_op\n    self._dtype)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 550, in read_variable_op\n    \"ReadVariableOp\", resource=resource, dtype=dtype, name=name)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nFailedPreconditionError (see above for traceback): Error while reading resource variable global_step from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/global_step)\n\t [[node global_step/Read/ReadVariableOp (defined at <ipython-input-4-76a7a9e69621>:3) ]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Error while reading resource variable global_step from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/global_step)\n\t [[{{node global_step/Read/ReadVariableOp}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-76a7a9e69621>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0m_gs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_gs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Error while reading resource variable global_step from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/global_step)\n\t [[node global_step/Read/ReadVariableOp (defined at <ipython-input-4-76a7a9e69621>:3) ]]\n\nCaused by op 'global_step/Read/ReadVariableOp', defined at:\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-76a7a9e69621>\", line 3, in <module>\n    global_step = tf.train.get_or_create_global_step()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/training_util.py\", line 162, in get_or_create_global_step\n    global_step_tensor = create_global_step(graph)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/training_util.py\", line 145, in create_global_step\n    ops.GraphKeys.GLOBAL_STEP])\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1479, in get_variable\n    aggregation=aggregation)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1220, in get_variable\n    aggregation=aggregation)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 547, in get_variable\n    aggregation=aggregation)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 499, in _true_getter\n    aggregation=aggregation)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 911, in _get_single_variable\n    aggregation=aggregation)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 213, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 176, in _variable_v1_call\n    aggregation=aggregation)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 155, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2488, in default_variable_creator\n    import_scope=import_scope)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 217, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 294, in __init__\n    constraint=constraint)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 446, in _init_from_args\n    value = self._read_variable_op()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 728, in _read_variable_op\n    self._dtype)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 550, in read_variable_op\n    \"ReadVariableOp\", resource=resource, dtype=dtype, name=name)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nFailedPreconditionError (see above for traceback): Error while reading resource variable global_step from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/global_step)\n\t [[node global_step/Read/ReadVariableOp (defined at <ipython-input-4-76a7a9e69621>:3) ]]\n"
     ]
    }
   ],
   "source": [
    "tmp = tf.constant([[[1,2,3,4], [5,6,7,8]]])\n",
    "\n",
    "def loss_function(pred, real):\n",
    "    \"\"\"\n",
    "    Since the target sequences are padded,\n",
    "    it is important to apply a padding mask when calculating the loss.\n",
    "    \"\"\"\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=real)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)    \n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative log likelihood loss. It is useful to train a classification problem with `C` classes.\n",
    "\n",
    "Computes sparse softmax cross entropy between `logits` and `labels`. Measures the probability error in discrete classification tasks in which the\n",
    "  classes are mutually exclusive (each entry is in exactly one class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "print(session.run(tf.global_variables_initializer()))\n",
    "print(session.run(tf.report_uninitialized_variables()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [2 1 0]]\n",
      "[[[0.93333334 0.03333334 0.03333334]\n",
      "  [0.03333334 0.93333334 0.03333334]\n",
      "  [0.93333334 0.03333334 0.03333334]]\n",
      "\n",
      " [[0.03333334 0.03333334 0.93333334]\n",
      "  [0.03333334 0.93333334 0.03333334]\n",
      "  [0.93333334 0.03333334 0.03333334]]]\n",
      "(2, 3)\n",
      "13.498105\n"
     ]
    }
   ],
   "source": [
    "from ops.regularization import label_smoothing\n",
    "\n",
    "\n",
    "ids = tf.constant([[0, 1, 0], [2, 1, 0]])\n",
    "\n",
    "lgts = tf.constant([[[12.2, 20.2, 23.3], [2.2, 4.2, 12.1], [2.2, 4.2, 12.1]], [[10.1, 42.1, 24.1], [2.1, 5.1, 10.1], [10.1, 42.1, 24.1]]])\n",
    "\n",
    "smooth_labels = label_smoothing(tf.one_hot(ids, depth=3))\n",
    "\n",
    "\n",
    "# use right shifted target\n",
    "lss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=lgts, labels=smooth_labels)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(ids))\n",
    "    print(sess.run(smooth_labels))\n",
    "    print(sess.run(lss).shape)\n",
    "    print(sess.run(tf.reduce_mean(lss)))\n",
    "        \n",
    "#     print(sess.run())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.bert import BertLayer\n",
    "\n",
    "\n",
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom Keras layer, integrating BERT from tf-hub\n",
    "    \"\"\"\n",
    "    def __init__(self, url, seq_len=512, d_embedding=768, n_fine_tune_layers=None, **kwargs):\n",
    "        self.url = url\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.seq_len = seq_len\n",
    "        self.d_embedding = d_embedding\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.url,\n",
    "            trainable=self.trainable,\n",
    "            name=\"{}_bert_module\".format(self.name)\n",
    "        )\n",
    "\n",
    "        trainable_vars = self.bert.variables\n",
    "        \n",
    "        if self.trainable:\n",
    "\n",
    "            # Remove unused layers\n",
    "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "\n",
    "            # Select how many layers to fine tune\n",
    "            trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
    "\n",
    "            # Add to trainable weights\n",
    "            for var in trainable_vars:\n",
    "                self._trainable_weights.append(var)\n",
    "\n",
    "            for var in self.bert.variables:\n",
    "                if var not in self._trainable_weights:\n",
    "                    self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        \n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        \n",
    "        bert_inputs = dict(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids)\n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\"sequence_output\"]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        raise ValueError()\n",
    "        return (input_shape[0], self.seq_len, self.d_embedding)\n",
    "\n",
    "    \n",
    "    \n",
    "in_id = tf.keras.layers.Input(shape=(SEQ_LEN,), name=\"input_ids\")\n",
    "in_mask = tf.keras.layers.Input(shape=(SEQ_LEN,), name=\"input_masks\")\n",
    "in_segment = tf.keras.layers.Input(shape=(SEQ_LEN,), name=\"segment_ids\")\n",
    "\n",
    "bert_inputs = [in_id, in_mask, in_segment]\n",
    "\n",
    "bert = BertLayer(BERT_MODEL_URL, trainable=False)(bert_inputs)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=bert_inputs, outputs=bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "atmp1 = np.zeros((30522))\n",
    "atmp1[10] = 1\n",
    "\n",
    "atmp2 = np.zeros((30522))\n",
    "atmp2[20] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atmp = np.array([atmp1, atmp2], dtype=np.float32)\n",
    "atmp.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.initializers import Constant\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    initialize_vars(sess)\n",
    "    embedding_matrix = sess.run(bert.variable_map['bert/embeddings/word_embeddings'])\n",
    "\n",
    "target_vocab_size = len(data)\n",
    "d_model = 768\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(target_vocab_size, d_model, trainable=False, embeddings_initializer=Constant(embedding_matrix))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(10), Dimension(30522), Dimension(768)])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.compute_output_shape((10, target_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_20:0\", shape=(2, 30522), dtype=float32)\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Resource localhost/embedding_5/embeddings/N10tensorflow3VarE does not exist.\n\t [[node embedding_5/embedding_lookup (defined at <ipython-input-52-fac9b5b8d76d>:6) ]]\n\nCaused by op 'embedding_5/embedding_lookup', defined at:\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/ioloop.py\", line 759, in _run_callback\n    ret = callback()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-52-fac9b5b8d76d>\", line 6, in <module>\n    print(sess.run(embedding(atmp_)).shape)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 554, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/layers/embeddings.py\", line 179, in call\n    out = embedding_ops.embedding_lookup(self.embeddings, inputs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 316, in embedding_lookup\n    transform_fn=None)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 133, in _embedding_lookup_and_transform\n    result = _clip(array_ops.gather(params[0], ids, name=name),\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 3271, in gather\n    return params.sparse_read(indices, name=name)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 759, in sparse_read\n    self._handle, indices, dtype=self._dtype, name=name)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 633, in resource_gather\n    validate_indices=validate_indices, name=name)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nNotFoundError (see above for traceback): Resource localhost/embedding_5/embeddings/N10tensorflow3VarE does not exist.\n\t [[node embedding_5/embedding_lookup (defined at <ipython-input-52-fac9b5b8d76d>:6) ]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Resource localhost/embedding_5/embeddings/N10tensorflow3VarE does not exist.\n\t [[{{node embedding_5/embedding_lookup}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-fac9b5b8d76d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0matmp_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matmp_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matmp_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bert/embeddings/word_embeddings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Resource localhost/embedding_5/embeddings/N10tensorflow3VarE does not exist.\n\t [[node embedding_5/embedding_lookup (defined at <ipython-input-52-fac9b5b8d76d>:6) ]]\n\nCaused by op 'embedding_5/embedding_lookup', defined at:\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/ioloop.py\", line 759, in _run_callback\n    ret = callback()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-52-fac9b5b8d76d>\", line 6, in <module>\n    print(sess.run(embedding(atmp_)).shape)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 554, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/layers/embeddings.py\", line 179, in call\n    out = embedding_ops.embedding_lookup(self.embeddings, inputs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 316, in embedding_lookup\n    transform_fn=None)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 133, in _embedding_lookup_and_transform\n    result = _clip(array_ops.gather(params[0], ids, name=name),\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 3271, in gather\n    return params.sparse_read(indices, name=name)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 759, in sparse_read\n    self._handle, indices, dtype=self._dtype, name=name)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 633, in resource_gather\n    validate_indices=validate_indices, name=name)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nNotFoundError (see above for traceback): Resource localhost/embedding_5/embeddings/N10tensorflow3VarE does not exist.\n\t [[node embedding_5/embedding_lookup (defined at <ipython-input-52-fac9b5b8d76d>:6) ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    initialize_vars(sess) \n",
    "    atmp_ = tf.convert_to_tensor(atmp, np.float32)\n",
    "    print(atmp_)\n",
    "    print(sess.run(embedding(atmp_)).shape)\n",
    "    raise ValueError\n",
    "    print(sess.run(tf.matmul(atmp, bert.variable_map['bert/embeddings/word_embeddings'])))\n",
    "    print(sess.run(bert.variable_map['bert/embeddings/word_embeddings'](np.array([0, 1]))))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 512) (1, 512) (1, 512)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-368a1b3e12eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#     print(x0, '\\n', x1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtmp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#     print(sess.run(tmp2))  # (batch_size, input_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "iter = train_dataset.make_initializable_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iter.initializer)\n",
    "    initialize_vars(sess)    \n",
    "    tmp = sess.run(el)\n",
    "    x0, x1, x2, y0, y1, y2 = tmp\n",
    "    print(x0.shape, x1.shape, x2.shape)\n",
    "#     print(x0, '\\n', x1)\n",
    "    tmp2 = model.predict([x0, x1, x2])\n",
    "    print(tmp2)\n",
    "#     print(sess.run(tmp2))  # (batch_size, input_seq_len, d_model)    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.49737298  0.5550515  -0.13341121 ... -0.5284789  -0.61509997\n",
      "   -0.04155324]\n",
      "  [ 0.5157517   0.5635947  -0.09799525 ... -0.5260042  -0.62772596\n",
      "   -0.03477238]\n",
      "  [ 0.53440964  0.5700865  -0.05099516 ... -0.5282975  -0.63185966\n",
      "   -0.02259888]\n",
      "  ...\n",
      "  [ 0.39933348  0.4695823  -0.19357261 ... -0.49450147 -0.70106435\n",
      "   -0.01530197]\n",
      "  [ 0.41170758  0.47419053 -0.20090303 ... -0.5092494  -0.7242769\n",
      "    0.0037366 ]\n",
      "  [ 0.42569858  0.49202585 -0.1921086  ... -0.52926743 -0.73894566\n",
      "    0.00663666]]\n",
      "\n",
      " [[ 0.49737298  0.5550515  -0.13341121 ... -0.5284789  -0.61509997\n",
      "   -0.04155324]\n",
      "  [ 0.5157517   0.5635947  -0.09799525 ... -0.5260042  -0.62772596\n",
      "   -0.03477238]\n",
      "  [ 0.53440964  0.5700865  -0.05099516 ... -0.5282975  -0.63185966\n",
      "   -0.02259888]\n",
      "  ...\n",
      "  [ 0.39933348  0.4695823  -0.19357261 ... -0.49450147 -0.70106435\n",
      "   -0.01530197]\n",
      "  [ 0.41170758  0.47419053 -0.20090303 ... -0.5092494  -0.7242769\n",
      "    0.0037366 ]\n",
      "  [ 0.42569858  0.49202585 -0.1921086  ... -0.52926743 -0.73894566\n",
      "    0.00663666]]\n",
      "\n",
      " [[ 0.49737298  0.5550515  -0.13341121 ... -0.5284789  -0.61509997\n",
      "   -0.04155324]\n",
      "  [ 0.5157517   0.5635947  -0.09799525 ... -0.5260042  -0.62772596\n",
      "   -0.03477238]\n",
      "  [ 0.53440964  0.5700865  -0.05099516 ... -0.5282975  -0.63185966\n",
      "   -0.02259888]\n",
      "  ...\n",
      "  [ 0.39933348  0.4695823  -0.19357261 ... -0.49450147 -0.70106435\n",
      "   -0.01530197]\n",
      "  [ 0.41170758  0.47419053 -0.20090303 ... -0.5092494  -0.7242769\n",
      "    0.0037366 ]\n",
      "  [ 0.42569858  0.49202585 -0.1921086  ... -0.52926743 -0.73894566\n",
      "    0.00663666]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.49737298  0.5550515  -0.13341121 ... -0.5284789  -0.61509997\n",
      "   -0.04155324]\n",
      "  [ 0.5157517   0.5635947  -0.09799525 ... -0.5260042  -0.62772596\n",
      "   -0.03477238]\n",
      "  [ 0.53440964  0.5700865  -0.05099516 ... -0.5282975  -0.63185966\n",
      "   -0.02259888]\n",
      "  ...\n",
      "  [ 0.39933348  0.4695823  -0.19357261 ... -0.49450147 -0.70106435\n",
      "   -0.01530197]\n",
      "  [ 0.41170758  0.47419053 -0.20090303 ... -0.5092494  -0.7242769\n",
      "    0.0037366 ]\n",
      "  [ 0.42569858  0.49202585 -0.1921086  ... -0.52926743 -0.73894566\n",
      "    0.00663666]]\n",
      "\n",
      " [[ 0.49737298  0.5550515  -0.13341121 ... -0.5284789  -0.61509997\n",
      "   -0.04155324]\n",
      "  [ 0.5157517   0.5635947  -0.09799525 ... -0.5260042  -0.62772596\n",
      "   -0.03477238]\n",
      "  [ 0.53440964  0.5700865  -0.05099516 ... -0.5282975  -0.63185966\n",
      "   -0.02259888]\n",
      "  ...\n",
      "  [ 0.39933348  0.4695823  -0.19357261 ... -0.49450147 -0.70106435\n",
      "   -0.01530197]\n",
      "  [ 0.41170758  0.47419053 -0.20090303 ... -0.5092494  -0.7242769\n",
      "    0.0037366 ]\n",
      "  [ 0.42569858  0.49202585 -0.1921086  ... -0.52926743 -0.73894566\n",
      "    0.00663666]]\n",
      "\n",
      " [[ 0.49737298  0.5550515  -0.13341121 ... -0.5284789  -0.61509997\n",
      "   -0.04155324]\n",
      "  [ 0.5157517   0.5635947  -0.09799525 ... -0.5260042  -0.62772596\n",
      "   -0.03477238]\n",
      "  [ 0.53440964  0.5700865  -0.05099516 ... -0.5282975  -0.63185966\n",
      "   -0.02259888]\n",
      "  ...\n",
      "  [ 0.39933348  0.4695823  -0.19357261 ... -0.49450147 -0.70106435\n",
      "   -0.01530197]\n",
      "  [ 0.41170758  0.47419053 -0.20090303 ... -0.5092494  -0.7242769\n",
      "    0.0037366 ]\n",
      "  [ 0.42569858  0.49202585 -0.1921086  ... -0.52926743 -0.73894566\n",
      "    0.00663666]]]\n"
     ]
    }
   ],
   "source": [
    "iter = train_dataset.make_initializable_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "#     sess.run(iter.initializer)\n",
    "#     tmp = sess.run(el)\n",
    "#     print(tmp)\n",
    "    sample_transformer = Transformer(\n",
    "        num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "        input_vocab_size=8500, target_vocab_size=8000)\n",
    "\n",
    "    temp_input = tf.random.uniform((64, 62))\n",
    "    temp_target = tf.random.uniform((64, 26))\n",
    "\n",
    "    fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
    "                                   enc_padding_mask=None, \n",
    "                                   look_ahead_mask=None,\n",
    "                                   dec_padding_mask=None)\n",
    "    initialize_vars(sess)\n",
    "    \n",
    "    print(sess.run(fn_out))  # (batch_size, input_seq_len, d_model)    \n",
    "#     x0, x1, x2, y0, y1, y2 = tmp\n",
    "#     tmp2 = model.predict([x0, x1, x2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8568d044b116>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# so their values get averaged.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtemp_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (3, 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mprint_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-8568d044b116>\u001b[0m in \u001b[0;36mprint_out\u001b[0;34m(q, k, v)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtemp_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Attention weights are:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtemp_attn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Output is:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtemp_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \"\"\"\n\u001b[0;32m--> 695\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5165\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msession\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5167\u001b[0;31m       raise ValueError(\"Cannot evaluate tensor using `eval()`: No default \"\n\u001b[0m\u001b[1;32m   5168\u001b[0m                        \u001b[0;34m\"session is registered. Use `with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5169\u001b[0m                        \u001b[0;34m\"sess.as_default()` or pass an explicit session to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`"
     ]
    }
   ],
   "source": [
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(q, k, v, None)\n",
    "    print ('Attention weights are:')\n",
    "    print (temp_attn.eval())\n",
    "    print ('Output is:')\n",
    "    print (temp_out.eval())\n",
    "\n",
    "    \n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This query aligns equally with the first and second key, \n",
    "# so their values get averaged.\n",
    "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "print_out(temp_q, temp_k, temp_v)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultiHeadAttention' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3149abf8f150>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtemp_mha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, encoder_sequence, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_mha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MultiHeadAttention' is not defined"
     ]
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "print(out.shape, attn.shape)\n",
    "\n",
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_ffn(tf.random.uniform((64, 50, 512)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(64), Dimension(26), Dimension(8000)])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    input_vocab_size=8500, target_vocab_size=8000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 62))\n",
    "temp_target = tf.random.uniform((64, 26))\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v1.keras.optimizers' has no attribute 'schedules'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-a3099a9cd8d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCustomSchedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLearningRateSchedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"\n\u001b[1;32m      3\u001b[0m     \u001b[0mUse\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mAdam\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcustom\u001b[0m \u001b[0mlearning\u001b[0m \u001b[0mrate\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0maccording\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mformula\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v1.keras.optimizers' has no attribute 'schedules'"
     ]
    }
   ],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    Use the Adam optimizer with a custom learning rate scheduler according to the formula\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sparse_categorical_crossentropy() missing 2 required positional arguments: 'y_true' and 'y_pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-22b0ad8ea901>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_categorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: sparse_categorical_crossentropy() missing 2 required positional arguments: 'y_true' and 'y_pred'"
     ]
    }
   ],
   "source": [
    "loss_object = tf.keras.losses.sparse_categorical_crossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    \"\"\"\n",
    "    Since the target sequences are padded, it is important to apply a padding mask when calculating the loss.\n",
    "    \"\"\"\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 511)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0[:, :-1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(64), Dimension(26), Dimension(512)]),\n",
       " TensorShape([Dimension(64), Dimension(8), Dimension(26), Dimension(62)]),\n",
       " TensorShape([Dimension(64), Dimension(8), Dimension(26), Dimension(26)]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=8000)\n",
    "\n",
    "output, attn = sample_decoder(tf.random.uniform((64, 26)), \n",
    "                              enc_output=sample_encoder_output, \n",
    "                              training=False, look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape, attn['decoder_layer2_block1'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, input_vocab_size=8500)\n",
    "\n",
    "sample_encoder_output = sample_encoder(tf.random.uniform((64, 62)), \n",
    "                                       training=False, mask=None)\n",
    "\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(64), Dimension(50), Dimension(512)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0523 12:59:35.651068 140526081922880 saver.py:1489] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "in_id = tf.keras.layers.Input(shape=(SEQ_LEN,), name=\"input_ids\")\n",
    "in_mask = tf.keras.layers.Input(shape=(SEQ_LEN,), name=\"input_masks\")\n",
    "in_segment = tf.keras.layers.Input(shape=(SEQ_LEN,), name=\"segment_ids\")\n",
    "\n",
    "bert_inputs = [in_id, in_mask, in_segment]\n",
    "\n",
    "bert = BertLayer(BERT_MODEL_URL, trainable=False)(bert_inputs)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=bert_inputs, outputs=bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    \"\"\"\n",
    "    Mask all the pad tokens in the batch of sequence.\n",
    "    It ensures that the model does not treat padding as the input.\n",
    "    The mask indicates where pad value 0 is present:\n",
    "    it outputs a 1 at those locations, and a 0 otherwise.    \n",
    "    \"\"\"\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions so that we can add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    \"\"\"\n",
    "    The look-ahead mask is used to mask the future tokens in a sequence.\n",
    "    In other words, the mask indicates which entries should not be used. \n",
    "\n",
    "    This means that to predict the third word, only the first and second word will be used.\n",
    "    Similarly to predict the fourth word, only the first, second and the third word will be used and so on.\n",
    "    \"\"\"\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1, 1, 5)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(create_padding_mask(x)).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'strided_slice_9:0' shape=(1, 1, 1, 512) dtype=float32>,\n",
       " <tf.Tensor 'Maximum_1:0' shape=(1, 1, 512, 512) dtype=float32>,\n",
       " <tf.Tensor 'strided_slice_10:0' shape=(1, 1, 1, 512) dtype=float32>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_masks(x0, y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101,  2028,  1997,  3725,  1005,  1055,  2087,  6450,  2406,\n",
       "         5014,  2038,  2908,  2006,  1996,  3006,  1011,  1998,  2009,\n",
       "         2428,  2003,  4906,  2005,  1037,  3035,  1012,  1996,  3694,\n",
       "         1045,  3205,  2026, 13629,  2380,  1010,  2275,  1999,  1037,\n",
       "         1015,  1010,  8380,  1011,  7456,  3776,  1010,  2038,  1037,\n",
       "         2157,  2548,  3976,  6415,  1011,  1037, 22771, 14534,  2629,\n",
       "         2213,  1012,  2021,  1996,  3776,  6074,  8304,  1996,  5096,\n",
       "         2360,  2045,  2003,  2053, 15843,  1997,  4022, 17394,  5782,\n",
       "         2000,  3193,  1996,  3200,  1012,  2188,  4906,  2005, 16664,\n",
       "         1024,  2026, 13629,  2380,  2003,  3694,  1045,  3205,  1998,\n",
       "         2001,  3294, 10601,  2058,  1037,  2093,  2095,  2622,  2011,\n",
       "         1996,  2783,  5608,  1012,  1996,  3035,  2218,  1037, 12695,\n",
       "         2182,  1999,  3541, 21136,  7430,  2005,  1017,  1010,  2199,\n",
       "         6368,  1999,  2494,  1012,  2009,  2003,  2028,  1997,  3725,\n",
       "         1005,  1055,  2087,  6450,  2406,  5014,  1998,  2038,  2908,\n",
       "         2006,  1996,  3006,  2005, 14534,  2629, 19912,  3258,  1012,\n",
       "         7010,  5226,  1010,  1996,  8228,  4944,  1010, 14105,  1996,\n",
       "         6418,  2164,  1996,  3829,  1998,  2817,  5331,  2007,  2797,\n",
       "        17692,  3871,  1012,  1520,  1996,  2332,  1521,  1055,  2534,\n",
       "         2001,  2649,  2011, 24794,  2271, 21877, 15088,  3678,  1006,\n",
       "         6549,  5272,  1007,  2004,  1005,  1996, 10418,  2282,  1999,\n",
       "        21136,  7430,  1005,  1010,  1998,  2038,  6565,  5894,  7535,\n",
       "         1998,  9487, 15673,  2147,  1012,  2009,  3138,  2039,  1037,\n",
       "         2312, 10817,  1997,  1996,  2160,  1012,  3205,  1024,  1996,\n",
       "         2160,  2038,  2260, 18390,  1010,  2184,  1997,  2068,  4372,\n",
       "        28880,  2229,  1010,  1037,  5059,  2282,  1010,  3075,  1010,\n",
       "         2817,  1998,  2851,  2282,  1012,  1996, 22445,  3040,  5010,\n",
       "         2038,  2048, 11225,  4734,  1010,  5723,  1998,  6457,  1012,\n",
       "         2110,  2135,  1024,  1996,  3776,  6074,  8304,  1996,  5096,\n",
       "         2360,  2045,  2003,  2053, 15843,  1997,  4022, 17634,  1012,\n",
       "         1996,  3668,  3829,  2003,  1037,  6919,  2155,  2686,  2007,\n",
       "         7759,  1998,  3564,  2181,  2007,  2341,  2041,  2000,  1996,\n",
       "         3871,  1012,  1998,  1996,  8546,  2778,  2064,  2202,  2039,\n",
       "         2000,  2176,  2847,  2138,  2045,  1005,  1055,  2061,  2172,\n",
       "         2000,  2156,  1012,  1996,  3035,  2218,  1037, 12695,  2012,\n",
       "         1996,  3200,  1999,  3541, 21136,  7430,  2005,  1017,  1010,\n",
       "         2199,  6368,  1012, 14675, 10239,  1010,  2132,  1997,  8623,\n",
       "         1998,  8707,  2005,  5000,  3581,  3776,  6074,  1010,  2056,\n",
       "         1024,  1005,  2009,  2003,  1037,  4310,  3200,  1010,  2028,\n",
       "         1997,  1996, 10418,  5973,  1999,  1996,  2406,  1012,  1005,\n",
       "         2057,  2024,  2893,  3037,  2013,  2248,  7846,  1010,  2013,\n",
       "         2789,  2885,  1998,  2060,  3033,  1997,  1996,  2088,  1012,\n",
       "         1005,  1996,  7759,  2282, 14567,  2049,  2434,  5997,  2989,\n",
       "         1010,  2038,  1037, 13788,  2012,  2593,  2203,  1998,  2052,\n",
       "        18579,  2835,  2012,  2560,  2484,  2111,  1012,  1996,  8546,\n",
       "         2778,  1997,  1996,  2160,  2064,  2202,  2039,  2000,  2176,\n",
       "         2847,  2138,  2045,  1005,  1055,  2061,  2172,  2000,  2156,\n",
       "         1012,  1996,  5059,  2282,  1010,  2125,  1996,  2307,  2534,\n",
       "         1010,  2003,  2028,  1997,  1996,  2116, 14036,  7258,  1999,\n",
       "         1996,  2160,  1012,  2026, 13629,  2380,  5246,  2067,  2000,\n",
       "        11502,  2692,  1998,  2001,  2320,  1996,  2188,  1997,  4787,\n",
       "         1052,  6672,  1010,  4905,  1011,  2236,  2000,  2798,  1045,\n",
       "         1012,  1996,  3075,  2282, 15646,  5328,  2058,  1996,  4193,\n",
       "        10833,  1012,  2021,  1996,  2160,  2036,  2038,  2260, 18390,\n",
       "         1010,  2184,  1997,  2068,  4372, 28880,  2229,  1010,  1037,\n",
       "         5059,  2282,  1010,  3075,  1010,  2817,  1010,  2851,  2282,\n",
       "         1010,  3021, 14619,  2282,  1010,  2399,  2282]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0528 14:56:33.385675 139957305521984 dataset_builder.py:177] Load pre-computed datasetinfo (eg: splits) from bucket.\n",
      "I0528 14:56:33.650062 139957305521984 dataset_info.py:377] Loading info from GCS for ted_hrlr_translate/pt_to_en/0.0.1\n",
      "I0528 14:56:34.125548 139957305521984 dataset_builder.py:236] Generating dataset ted_hrlr_translate (/home/ec2-user/tensorflow_datasets/ted_hrlr_translate/pt_to_en/0.0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset ted_hrlr_translate (124.94 MiB) to /home/ec2-user/tensorflow_datasets/ted_hrlr_translate/pt_to_en/0.0.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6dfc945abf4e8db29e012e3edf59f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Dl Completed...', max=1, style=ProgressStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322f001684f943b5a4bd96f385d9e393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Dl Size...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2754661cd1404eab89a209a68c687e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Extraction completed...', max=1, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0528 14:56:34.203127 139957305521984 download_manager.py:241] Downloading http://www.phontron.com/data/qi18naacl-dataset.tar.gz into /home/ec2-user/tensorflow_datasets/downloads/phontron.com_qi18naacl-datasetLVhDyvf9PQ-GjP4jim31ESZuvRjHI5wpNhR5SJvsNOo.tar.gz.tmp.85acbe6968a948c0ab60989f292e5947...\n",
      "I0528 14:56:46.151685 139957305521984 dataset_builder.py:675] Generating split train\n",
      "I0528 14:56:46.155997 139957305521984 file_format_adapter.py:271] Writing TFRecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20690b6bc9b14ea0aff1cd5ab3ec079d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae1406d2d9f4c42ba3b338d142995b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Shuffling...', max=1, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_datasets/core/file_format_adapter.py:247: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0528 14:56:52.763825 139957305521984 deprecation.py:323] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_datasets/core/file_format_adapter.py:247: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5442f9e3b56a46f0b06531ab15857bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1718689923304bb088ae2ca70a42626b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=51785, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0528 14:56:53.236819 139957305521984 dataset_builder.py:675] Generating split validation\n",
      "I0528 14:56:53.238343 139957305521984 file_format_adapter.py:271] Writing TFRecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5001f59ee4244472828d2bde386ec544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8832b4d6042745e9b86586920bac8ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Shuffling...', max=1, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2477f8c0d1774825bc5e3450f2268e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fe350d7e8b45cf9bfac83446af7abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=1193, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0528 14:56:53.504857 139957305521984 dataset_builder.py:675] Generating split test\n",
      "I0528 14:56:53.506487 139957305521984 file_format_adapter.py:271] Writing TFRecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4168a25eb07a4d5a9cc12e37115ac347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6556ab0e53a4233a9a486272b8af817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Shuffling...', max=1, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ccf568f988e4526b750fbfafb87b076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Reading...', max=1, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38273357a0624326b45f6552b97f96e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Writing...', max=1803, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0528 14:56:53.876027 139957305521984 dataset_builder.py:264] Skipping computing stats for mode ComputeStatsMode.AUTO.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset ted_hrlr_translate downloaded and prepared to /home/ec2-user/tensorflow_datasets/ted_hrlr_translate/pt_to_en/0.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True, as_supervised=True)\n",
    "\n",
    "train_examples, val_examples = examples['train'], examples['validation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os astrónomos acreditam que cada estrela da galáxia tem um planeta , e especulam que até um quinto deles tem um planeta do tipo da terra que poderá ter vida , mas ainda não vimos nenhum deles .\n",
      "\n",
      "astronomers now believe that every star in the galaxy has a planet , and they speculate that up to one fifth of them have an earth-like planet that might be able to harbor life , but we have n't seen any of them .\n"
     ]
    }
   ],
   "source": [
    "iter = train_examples.make_initializable_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iter.initializer)  \n",
    "    x, y = sess.run(el)\n",
    "    print(x.decode('utf-8'))\n",
    "    print()\n",
    "    print(y.decode('utf-8'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
